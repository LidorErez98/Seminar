{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad91a466-666e-4cf8-b529-583b0b5b6d95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, Model, losses\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, concatenate, Input, Lambda,Dropout,Conv2DTranspose\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288f9d3b-6694-4d1a-8005-a942f914c923",
   "metadata": {},
   "source": [
    "# Load NPZ files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908b07f-6e56-475e-863a-9a541a17e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = '../stack arrays/'\n",
    "def loadStackArray(path):\n",
    "    data = np.load(path)\n",
    "    images, labels = data['images'], data['labels']\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def GetFinalData(path_2017,path_2020):\n",
    "    print(\"Loading File...\")\n",
    "    images, labels = loadStackArray(path_2017)\n",
    "    images_20, labels_20 = loadStackArray(path_2020)\n",
    "    print(\"Stacking Images...\")\n",
    "    final_images = np.vstack([images, images_20])\n",
    "    del images_20, images\n",
    "    print(\"Stacking Labels...\")\n",
    "    final_labels = np.vstack([labels, labels_20])\n",
    "    return final_images, final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e729ef4-f277-4579-8f72-8613cc88b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = GetFinalData(orig+'training_stacked_arrays_2017.npz', orig+'training_stacked_arrays_2020.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003de99-98e3-4d96-878b-381ef5b57c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images, val_labels = GetFinalData(orig+'validation_stacked_arrays_2017.npz', orig+'validation_stacked_arrays_2020.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f1fca-dc4c-4d5e-90fb-8f685c0d17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a817924-215b-4744-8a9c-05f5ee545169",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f138a-c041-47a7-9be9-19ebb480b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels/255.0\n",
    "val_labels = val_labels/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eabd5b-ab8c-4568-8dbc-0e019cea9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    train_images, train_labels = tf.convert_to_tensor(train_images), tf.convert_to_tensor(train_labels)\n",
    "    val_images, val_labels = tf.convert_to_tensor(val_images), tf.convert_to_tensor(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e19f3ea-bfdc-4758-8580-e3010e17f966",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9952ea54-5075-4e3d-a8d4-ea9ecd2fb45b",
   "metadata": {},
   "source": [
    "def build_unet(img_width=img_width, img_height=img_height,img_channels=img_channels):\n",
    "\n",
    "    inputs = tf.keras.layers.Input((img_width, img_height, img_channels)) \n",
    "    normalized_inputs = Lambda(lambda x: x / 255.0)(inputs)\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer ='he_normal', padding='same')(normalized_inputs)\n",
    "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "    \n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "    \n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "    \n",
    "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    p5 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c5)\n",
    "    \n",
    "    c6 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p5)\n",
    "    c6 = tf.keras.layers.Dropout(0.3)(c6)\n",
    "    c6 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "    #p6 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c6)\n",
    "    \n",
    "    # Expansive path\n",
    "    u7 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = tf.keras.layers.concatenate([u7, c5])\n",
    "    c7 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
    "    c7 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "    \n",
    "    u8 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = tf.keras.layers.concatenate([u8, c4])\n",
    "    c8 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = tf.keras.layers.Dropout(0.2)(c8)\n",
    "    c8 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "    \n",
    "    u9 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = tf.keras.layers.concatenate([u9, c3])\n",
    "    c9 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = tf.keras.layers.Dropout(0.2)(c9)\n",
    "    c9 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "    \n",
    "    u10 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c9)\n",
    "    u10 = tf.keras.layers.concatenate([u10, c2])\n",
    "    c10 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u10)\n",
    "    c10 = tf.keras.layers.Dropout(0.1)(c10)\n",
    "    c10 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c10)\n",
    "    \n",
    "    u11 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c10)\n",
    "    u11 = tf.keras.layers.concatenate([u11, c1], axis=3)\n",
    "    c11 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u11)\n",
    "    c11 = tf.keras.layers.Dropout(0.1)(c11)\n",
    "    c11 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c11)\n",
    "    \n",
    "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c11)\n",
    "    unet_model = Model(inputs=[inputs], outputs=[outputs], name='U-Net')\n",
    "    return unet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9fffd5-33c2-4b74-8466-c3f434fb8472",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46b274-ea83-4e59-814a-8c5dfedca0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focalLoss(alpha=0.25, gamma=2.0):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "        \n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_weight = alpha_factor * tf.pow((1 - p_t), gamma)\n",
    "        bce = K.binary_crossentropy(y_true, y_pred)\n",
    "        \n",
    "        loss = focal_weight * bce\n",
    "        return loss  \n",
    "    return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509f770-e696-472b-a384-073a02b5efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diceLoss(y_true, y_pred):\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "    \n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "    union = tf.reduce_sum(y_true + y_pred, axis=[1, 2, 3])\n",
    "    \n",
    "    smooth = 1e-6  # Adding a small constant to avoid division by zero\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - dice  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd798a-391a-4ec6-8d7e-d3963ddd7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(alpha=0.25, gamma=2.0, focal_weight=0.5, dice_weight=0.5, **kwargs):\n",
    "    focal_loss = focalLoss(alpha=alpha, gamma=gamma)\n",
    "    \n",
    "    def combined(y_true, y_pred):\n",
    "        focal = focal_loss(y_true, y_pred)\n",
    "        dice = diceLoss(y_true, y_pred)\n",
    "        \n",
    "        combined_loss = focal_weight * focal + dice_weight * dice\n",
    "        \n",
    "        return tf.reduce_mean(combined_loss)\n",
    "    \n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a84b4-9e70-4cc6-bee1-b21001525980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if TensorFlow recognizes the GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# List available devices\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdcc770-f115-4ffc-b245-2424b608ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 256\n",
    "img_height = 256\n",
    "img_channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45a8db-5545-4760-b1d7-24245f2715c4",
   "metadata": {},
   "source": [
    "# Original Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be60c54c-f277-49ab-b020-14625a422849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EncoderBlock(x, filters, useMaxPool=True, dropout = 0.1):\n",
    "    c1 = Conv2D(filters, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    c2 = Conv2D(filters, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    if useMaxPool:\n",
    "        p = MaxPooling2D((2,2))(c2)\n",
    "    else:\n",
    "        p = c2\n",
    "    if dropout > 0:\n",
    "        d = Dropout(dropout)(p)\n",
    "    else:\n",
    "        d = p\n",
    "    return d,c2\n",
    "\n",
    "def DecoderBlock(x,out, filters, dropout = 0.3):\n",
    "    C_Transpose = Conv2DTranspose(filters,(2, 2), strides=(2, 2), padding='same')(x)\n",
    "    merge = concatenate([C_Transpose, out], axis=3)\n",
    "    c = Conv2D(filters,(3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(merge)\n",
    "    c = Conv2D(filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c)\n",
    "    if dropout > 0:\n",
    "        d = Dropout(dropout)(c)\n",
    "    else:\n",
    "        d = c\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1678d2d1-12f7-4294-b6c2-4a4052c7ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNET(img_width, img_height, img_channels):\n",
    "    inputs = Input((img_width, img_height, img_channels))\n",
    "    normalized_inputs = Lambda(lambda x: x / 255.0)(inputs)\n",
    "    # Contracting Path\n",
    "    d1,c1 = EncoderBlock(normalized_inputs,64)\n",
    "    d2,c2 = EncoderBlock(d1,128)\n",
    "    d3,c3 = EncoderBlock(d2,256, dropout=0.2)\n",
    "    d4,c4 = EncoderBlock(d3,512, dropout=0.2)\n",
    "    # Buttleneck\n",
    "    d5,c5 = EncoderBlock(d4,1024,False, dropout=0.3)\n",
    "    # Expansive Path\n",
    "    c6 = DecoderBlock(c5,c4, 512)\n",
    "    c7 = DecoderBlock(c6,c3,256,dropout=0.2)\n",
    "    c8 = DecoderBlock(c7,c2,128, dropout=0.2)\n",
    "    c9 = DecoderBlock(c8,c1,64,dropout=0.1)\n",
    "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    unet_model = Model(inputs=[inputs], outputs=[outputs], name='U-Net') \n",
    "    return unet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c7a7b-3256-446b-8857-b5e396ca3c41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# FIND Best GAMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c5108d-d9f8-421c-b486-00b0ac047505",
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [2,3,4]\n",
    "alpha = 0.25\n",
    "BATCH_SIZE=32\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=2, monitor='val_loss'),\n",
    "    TensorBoard(log_dir='logs'),\n",
    "    ModelCheckpoint('solar_panels.keras', save_best_only=True, monitor='val_loss')  \n",
    "]\n",
    "\n",
    "\n",
    "for gamma in gammas:\n",
    "    with tf.device('/CPU:0'):\n",
    "        unet = UNET(img_width, img_height, img_channels)\n",
    "        unet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-5),\n",
    "                      loss=build_loss(gamma=gamma, alpha=alpha),\n",
    "                      metrics=[tf.keras.metrics.Precision(),\n",
    "                               tf.keras.metrics.Recall(), \n",
    "                               keras.metrics.BinaryIoU(target_class_ids=[1], name='IoU')])\n",
    "        \n",
    "    with tf.device('/GPU:1'):\n",
    "        results = unet.fit(\n",
    "        train_images, train_labels,\n",
    "        epochs=10,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(val_images, val_labels), verbose=1)\n",
    "    # Save the training history\n",
    "    alpha_str = str(alpha).replace('.','')\n",
    "    with open(f'./unet_alpha_{alpha_str}_gamma_{gamma}.pkl', 'wb') as file:\n",
    "        pickle.dump(results.history, file)\n",
    "    unet.save(f'./unet_alpha_{alpha_str}_gamma_{gamma}.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdabb78a-49cb-4e2d-aa3c-08b90cd29dc6",
   "metadata": {},
   "source": [
    "# After finding best gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac4ec21-b568-4bfe-a1ef-1242c1c2676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 2 # best gamma\n",
    "alpha = 0.25 \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=2, monitor='val_loss'),\n",
    "    TensorBoard(log_dir='logs'),\n",
    "    ModelCheckpoint('solar_panels.keras', save_best_only=True, monitor='val_loss')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756f402-eade-4de8-b517-f92de7e8b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "        unet = UNET(img_width, img_height, img_channels)\n",
    "        unet.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss = build_loss(gamma=gamma, alpha=alpha),\n",
    "        metrics = [\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall(),\n",
    "            tf.keras.metrics.BinaryIoU(target_class_ids=[1], name='IoU') # IOU of class 1 only.\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a1d84-deb9-4637-812a-b9722bb5b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:1'):\n",
    "    results = unet.fit(\n",
    "        train_images, train_labels,\n",
    "        epochs=50,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(val_images, val_labels),\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee84ee-c450-47a0-88e2-a38d6362e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training history\n",
    "with open(f'./unet_alpha_025_gamma_2_final.pkl', 'wb') as file:\n",
    "    pickle.dump(results.history, file)\n",
    "# Save the final model\n",
    "unet.save(f'./UNET_alpha_025_gamma_2_final.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
